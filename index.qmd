---
title: "Weighted pairwise likelihood and limited information goodness-of-fit tests for binary factor models"
subtitle: "Department of Statistics and Data Science Seminar, NUS"
author:
  - name: Haziq Jamil
    orcid: 0000-0003-3298-1010
    affiliations: 
      - 'Assistant Professor in Statistics, Universiti Brunei Darussalam<br>Visiting Fellow, London School of Economics and Political Science'
      - '<span style="font-style:normal;">[`https://haziqj.ml/plgof-nus/`](https://haziqj.ml/plgof-nus/)</span>'
date: "18 October 2024"
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
library(lavaan.bingof)
library(lavaan)
library(semPlot)
library(gt)
library(tidyverse)
library(Matrix)
load("R/p_mult_bern_fig.RData")
```


```{r latex_shortcuts}
#| echo: false
#| results: asis
#| eval: !expr knitr::is_html_output()

# LaTeX shortcuts 
cat(readr::read_file("maths_shortcuts.tex"))
``` 


::: {.columns}

::: {.column width="50%"}
![Irini Moustaki<br>*London School of Economics and Political Science*](https://irmoustaki.github.io/Irini2022.jpg){height=330px}
:::

::: {.column width="50%"}
![Chris Skinner (1954-2020)<br>*London School of Economics and Political Science*](https://blogs.city.ac.uk/addresponse/files/2014/09/Christopher_Skinner_4431-1sqbwwa-200x300.jpg){height=330px}
:::

:::

<br>

::: {.nudge-up-xl}

> **Jamil, H.**, Moustaki, I., & Skinner, C. (2024). Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling. *Br. J. Math. Stat. Psychol., In Press* [arXiv.2311.02543](https://doi.org/10.48550/arXiv.2311.02543). URL: [`https://haziqj.ml/plgof-nus/`](https://haziqj.ml/plgof-nus/)

:::

# Introduction 

## Introduction

::: {.callout-note icon=false title="Context"}
Employ latent variable models (factor models) to analyse binary data $y_1,\dots,y_p$ collected via simple random or complex sampling.
:::

:::: {.columns}

::: {.column width="33.3%"}
![*(Psychometrics)*<br>Behavioural checklist](figures/eg1a.jpg)
:::

::: {.column width="33.3%"}
![*(Education)*<br>Achievement test](figures/eg2a.jpg)
:::

::: {.column width="33.3%"}
![*(Sociology)*<br>Intergenerational support](figures/eg3a.jpg)
:::

::::

::: {.aside}
Photo credits: Unsplash
[\@glenncarstenspeters](https://unsplash.com/photos/RLw-UC03Gwc),
[\@ivalex](https://unsplash.com/photos/PDRFeeDniCk),
[\@oanhmj](https://unsplash.com/photos/8uhVTxlbBd4)
:::


## Introduction (cont.)

::: {.fragment .fade-out fragment-index=1 .absolute left=0 top=80}
| $i$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ |
|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | 1 | 0 | 0 | 1 | 1 |
| 2 | 1 | 1 | 1 | 1 | 1 |
| 3 | 1 | 1 | 1 | 0 | 1 |
| $\vdots$ |  |  | $\vdots$ |  | |
| $n$ | 1 | 0 | 0 | 1 | 1 |
|  |  |  |  |  |  |
: {.table-ubd}
:::

::: {.fragment .fade-in-then-out fragment-index=1 .absolute left=0 top=80}
| $i$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ | Pattern |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | 1 | 0 | 0 | 1 | 1 | 10011 |
| 2 | 1 | 1 | 1 | 1 | 1 | 11111 |
| 3 | 1 | 1 | 1 | 0 | 1 | 11101 |
| $\vdots$ |  |  | $\vdots$ |  |  | $\vdots$ |
| $n$ | 1 | 0 | 0 | 1 | 1 | 10011 |
|  |  |  |  |  |  |  |
: {.table-ubd}
:::

::: {.fragment .fade-in-then-out fragment-index=2 .absolute left=0 top=80}
| $r$ | $y_1$ | $y_2$ | $y_3$ | $y_4$ | $y_5$ | Pattern | Obs. freq |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| 1 | 1 | 1 | 1 | 1 | 1 | 11111 | 343 |
| 2 | 1 | 1 | 0 | 1 | 1 | 11011 | 153 |
| 3 | 1 | 0 | 1 | 1 | 1 | 10111 | 71 |
| $\vdots$ |  |  | $\vdots$ |  |  | $\vdots$ | $\vdots$ |
| [$R$]{.ubdbluebg} | 0 | 1 | 1 | 1 | 0 | 01110 | 1 |
|  |  |  |  |  |  |  |  |
: {.table-ubd }
:::

::: {.fragment .fade-in-then-out fragment-index=2 .absolute left=250 top=500}
[$R = 2^p$]{.ubdbluebg}
:::


::: {.fragment .fade-in-then-out fragment-index=3 .absolute left=0 top=80}
| $r$ | Pattern | Obs. freq |
|:---:|:---:|:---:|
| 1 | 11111 | 343 |
| 2 | 11011 | 153 |
| 3 | 10111 | 71 |
| $\vdots$ | $\vdots$ | $\vdots$ |
| [32]{.ubdblue} | 01110 | 1 |
|  |  |  |
: {.table-ubd }
:::

::: {.fragment .fade-in-then-out fragment-index=4 .absolute left=0 top=80}
| $r$ | Pattern | Obs. freq | Exp. freq |
|:---:|:---:|:---:|:---:|
| 1 | 11111 | 343 | 342.1 |
| 2 | 11011 | 153 | 151.3 |
| 3 | 10111 | 71 | 62.81 |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| 32 | 01110 | 1 | 0.948 |
|  |  |  |  |
: {.table-ubd }
:::

::: {.fragment .fade-in-then-out fragment-index=5 .absolute left=0 top=80}
| $r$ | Pattern | Obs. freq | Exp. freq |
|:---:|:---:|:---:|:---:|
| 1 | 11111 | 343 | 342.1 |
| 2 | 11011 | 153 | 151.3 |
| 3 | 10111 | 71 | 62.81 |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| 28 | 01000 | [**1**]{.ubdredbg} | 1.831 |
| 29 | 01010 | [**1**]{.ubdredbg} | 3.276 |
| 30 | 01100 | [**1**]{.ubdredbg} | 0.948 |
| 31 | 01101 | [**0**]{.ubdredbg} | 0.013 |
| 32 | 01110 | [**0**]{.ubdredbg} | 0.009 |
|  |  |  |  |
: {.table-ubd }
:::

::: {.fragment .fade-in-then-out fragment-index=6 .absolute left=0 top=80}
| $r$ | Pattern | Obs. freq | [Exp. freq]{.ubdyellowbg} |
|:---:|:---:|:---:|:---:|
| 1 | 11111 | 343 | 342.1 |
| 2 | 11011 | 153 | 151.3 |
| 3 | 10111 | 71 | 62.81 |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| 28 | 01000 | 1 | 1.831 |
| 29 | 01010 | 1 | 3.276 |
| 30 | 01100 | 1 | 0.948 |
| 31 | 01101 | 0 | 0.013 |
| 32 | 01110 | 0 | 0.009 |
|  |  |  |  |
: {.table-ubd }
:::

::: {.fragment .fade-in fragment-index=7 .absolute left=0 top=80}
| $r$ | Pattern | [Obs. freq]{.ubdtealbg} | Exp. freq |
|:---:|:---:|:---:|:---:|
| 1 | 11111 | 360.9 | 342.1 |
| 2 | 11011 | 181.2 | 151.3 |
| 3 | 10111 | 68.05 | 62.81 |
| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| 28 | 01000 | 1.716 | 1.831 |
| 29 | 01010 | 1.120 | 3.276 |
| 30 | 01100 | 0.591 | 0.948 |
| 31 | 01101 | 0 | 0.013 |
| 32 | 01110 | 0 | 0.009 |
|  |  |  |  |
: {.table-ubd }
:::

::::: {.absolute left=600}
::: {.fragment fragment-index=5}
- [**Sparsity**]{.ubdredbg} affects reliability of goodness-of-fit tests.
  - Limited information tests [@reiser1996analysis; @bartholomew2002goodness;  @maydeu2005limited]
:::
:::::

::::: {.absolute left=600 top=315}
::: {.fragment fragment-index=6}
- [**Computational burden**]{.ubdyellowbg} of likelihood-based models.
  - Pairwise likelihood [@varin2011overview; @katsikatsou2012pairwise]
:::
:::::

::::: {.absolute left=600 top=532}
::: {.fragment fragment-index=7}
- [**Unequal probability sampling**]{.ubdtealbg} (e.g. due to a complex design)
  - Incorporate design weights [@skinner1989domain; @muthen1995complexa]
:::
:::::

## Definitions


- Let $\by = (y_1, \ldots, y_p)^\top \in\{0,1\}^p$ be a vector of Bernoulli random variables.

::: {.incremental .nudge-up-small}
- The joint probability of observing a response pattern $\bc_r=(c_{r1},\dots,c_{rp})^\top$, for each $r=1,\dots,R:=2^p$, is given by
$$
\pi_r = \Pr(\by = \bc_r) = \Pr(y_1=c_{r1},\dots,y_p=c_{rp}),
$${#eq-joint-bern-prob}
with $\sum_{r=1}^R \pi_r = 1$.

- Suppose observations $\cY := \big\{\by^{(h)}\big\}_{h=1}^n$  are obtained, where each unit $h$ has a probability of selection $1/w_h$. 

- Out of convenience, sampling weights are (typically) normalised so that $\sum_{h=1}^n w_h = n$.
  - Simple random sampling (SRS): $w_h=1, \forall h$.
  - Stratified sampling: $w_h = N_s(h)/N$, the stratum fractions.
  - Etc.
:::

## Multinomial distribution

- Let $p_r = \hat n_r \big/ \sum_h w_h$ be the the $r$th entry of the $R$-vector of proportions $\bp$, with $\hspace{1.4cm}$

::: {.nudge-up-small}
::: {.nudge-up-xl}
$$
\hat n_r = \sum_{h=1}^n w_h [\by^{(h)} = \bc_r].
$${#eq-obs-freq}
:::
:::


::: {.nudge-up-large}
::: {.fragment fragment-index=1 .nudge-up-xl}
- The random vector $\hat\bn = (\hat n_1,\dots,\hat n_R)^\top$ follows a multinomial distribution with parameters $n$, $R$, and $\bpi:=(\pi_1,\dots,\pi_R)^\top$, with
:::
:::


::: {.nudge-up-small}
::: {.nudge-up-xxl}
::: {.fragment fragment-index=1 .nudge-up-xl}
$$
\E(\hat\bn) = n\bpi \hspace{2em}\text{and}\hspace{2em} \Var(\hat\bn) = n\big( \ \myoverbrace{\diag(\bpi) - \bpi\bpi^\top}{\bSigma} \ \big).
$$
:::
:::
:::

::: {.nudge-up-small}
::: {.nudge-up-xl}
::: {.fragment .nudge-up-xxl}
- It is widely known that [@agresti2012categorical] for IID samples that
$$
\sqrt n (\bp - \bpi) \xrightarrow{\text D} {\N}_R(\bzero, \bSigma)
$${#eq-multin-clt}
as $n\to\infty$. This also works for complex sampling [@fuller2009introduction], but $\bSigma$ need not take a multinomial form.
:::
:::
:::

## Parametric models


::: {.columns}

::: {.column width=34%}

::: {.nudge-up-small}
$$
H_0: \bpi = \bpi(\btheta)
$$
:::

::: {.nudge-up-large}

![](figures/sem_path.png)

:::

:::

::: {.column width=66%}

::: {.absolute top=25}
- E.g. binary factor model with underlying variable approach (s.t. constraints)
$$
\begin{gathered}
y_i = \begin{cases}
1 & y_i^* > \tau_i \\
0 & y_i^* \leq \tau_i
\end{cases} \\
\by^* = \bLambda\bfeta + \bepsilon \\
\bfeta \sim {\N}_q(\bzero, \bPsi), \hspace{3pt}
\bepsilon \sim {\N}_p(\bzero, \bTheta_{\epsilon})
\end{gathered}
$${#eq-facmod}
:::

::: {.fragment fragment-index=1 .absolute top=355}
- The log-likelihood for 
$\btheta^\top = ($[$\blambda$]{.ubdred}$,\,$[$\brho$]{.ubdteal}$,\,$[$\btau$]{.ubdblue}$)\in\bbR^m$ is 
$$
\log L(\btheta \mid \cY) = \sum_{r=1}^R \hat n_r \log \pi_r(\btheta)
$${#eq-facmod-loglik}
where $\pi_r(\btheta) = \int_{\cC_r} \phi_p(\by^* \mid \bzero, \bLambda\bPsi\bLambda^\top + \bTheta_\epsilon) \dint \by^*$.
:::

::: {.fragment .fade-in .absolute top=643}
- \normalsize FIML may be difficult (high-dimensional integral; perfect separation).
:::

:::

:::

## Composite likelihood 

- *Terminology*: Pseudo-likelihood, quasi-likelihood (Ã  la @wedderburn1974quasilikelihood or misspecified models), limited information methods.

::: {.incremental .nudge-up}
- Let $\{\cA_1,\dots,\cA_K\}$ be a set of marginal or conditional events (partitioning the variable space). The composite likelihood is defined as [@lindsay1988composite]
$$
\cL(\btheta \mid \by) = \prod_{k=1}^K L(\btheta \mid \by \in \cA_k)^{\textcolor{lightgray}{\omega_k}}
$$

::: {.nudge-up-large}
- Component likelihoods $L(\btheta \mid \by \in \cA_k)$ are either [*conditional*]{.ubdbluebg} [@besag1974spatial;@liang1987extended;@molenberghs2006models] or [*marginal*]{.ubdredbg} [@chandler2007inference;@cox2004note;@varin2008composite] densities.

- Composite likelihood enjoys nice features [@varin2011overview]: relatively efficient, robust, and easier to maximise (smoother surface).
:::
:::

::::: {.notes}
- Composite likelihood is an inference function derived by multiplying a collection of component likelihoods

- Because each individual component is a conditional or marginal density, the resulting estimating equation obtained from the derivative of the composite log-likelihood is an unbiased estimating equation

- Because the components are multiplied, whether or not they are independent, the inference function has the properties of likelihood from a misspeciï¬ed model

- Terminology
  - Pseudo-likelihood: Function of parameter and data that behaves in "some respect" as a likelihood
  - Composite likelihood: An example of pseudo-likelihood, based on terms that are marginal or conditional densities
  - Quasi-likelihood: Wedderburn's likelihood which is not a pdf or misspecified likelihoods
  - Limited information methods: Inference procedures based on low-dimensional margins.



- Some history
  - Besag (1974) introduced the idea of composite likelihood for spatial data
  - Liang (1987) studies composite conditional likelihoods
  - Molenberghs and Verbeke (2005) in the context of longitudinal studies


  - Independence likelihood
  - Pairwise likelihood
:::::





## An analogy 

::: {.absolute top=30 left=125}
![](figures/Jigsaw.png){width=85%}
:::

::: {.fragment .fade-in fragment-index=1 .absolute top=30 left=125}
![](figures/stage1.png){width=85%}
:::

::: {.fragment .fade-in .absolute top=30 left=125}
![](figures/stage2.png){width=85%}
:::

::: {.fragment .fade-in .absolute top=30 left=125}
![](figures/stage3.png){width=85%}
:::

::: {.fragment .fade-in .absolute top=30 left=125}
![](figures/stage4.png){width=85%}
:::

::: {.fragment .fade-in .absolute top=30 left=125}
![](figures/stage5.png){width=85%}
:::

::: {.fragment .fade-in .absolute top=30 left=125}
![](figures/stage6.png){width=85%}
:::

::: {.fragment }

::: {.absolute top=30 left=125}
![](figures/full_jigsaw2.png){width=85%}
:::

::: {.absolute left=240 top=300}
[**_One may enjoy the approximate picture despite<br>not being able to see every blade of grass._**]{.ubdtealbg}
:::

:::

## Pairwise likelihood estimation

- For pairs of variables $y_i$ and $y_j$, $i,j=1,\dots,p$, and $i<j$, define $\hspace{5cm}$ 
$$
\pi_{cc'}^{(ij)}(\btheta) = \Pr_{\btheta}(y_i = c, y_j = c'), \hspace{2em} c,c'\in\{0,1\}.
$${#eq-pairwise}
There are $\tilde R = 4 \times \binom{p}{2}$ such probabilities, with $\sum_{c,c'} \pi_{cc'}^{(ij)}(\btheta) = 1$.

::: {.incremental .nudge-up}
- The pairwise log-likelihood takes the form [@katsikatsou2012pairwise]
$$
\log \pL(\btheta \mid \cY) = \sum_{i<j} \sum_{c}\sum_{c'} \hat n_{cc'}^{(ij)} \log \pi_{cc'}^{(ij)}(\btheta),
$${#eq-pairwise-loglik}
where $\hat n_{cc'}^{(ij)} = \sum_h w_h [\by^{(h)}_i = c, \by^{(h)}_j = c']$.

- The evaluation of @eq-pairwise-loglik now involves only bivariate normal integrals!
$$
\pi_{cc'}^{(ij)}(\btheta) = \iint_{\tilde{\cC}_{cc'}} \phi_2\big(\by^*_{ij} \mid \bzero, \bSigma_{y^*}^{(ij)} (\btheta)\big) \dint \by^*_{ij}
$$
:::

## MPLE properties

- Let $\mlepl = \argmax_{\btheta} \pL(\btheta \mid \cY)$. Under certain regularity conditions [@varin2011overview], as $n\to\infty$,
$$
\sqrt n (\mlepl - \btheta) \xrightarrow{\text D} {\N}_m
\left(
\bzero, 
\left\{ \cH(\btheta)\cJ(\btheta)^{-1}\cH(\btheta) \right\}^{-1}
\right),\hspace{1em} \text{where}
$${#eq-asymptotic-mple}
   - $\cH(\btheta)=-\E\big[\nabla^2\log \pL(\btheta \mid \by^{(h)})\big]$ is the *sensitivity matrix*; and
   - $\cJ(\btheta)=\Var\big[\nabla\log\pL(\btheta \mid \by^{(h)})\big]$ is the *variability matrix*.

::: {.fragment .nudge-up-small}
- Estimators of these matrices are given by [@zhao.joe05;@asparouhov2005sampling]

::: {.nudge-up-large}
$$
\begin{aligned}
\hat\bH &= - \frac{1}{\sum_h w_h} \sum_h \nabla^2\log \pL(\btheta \mid \by^{(h)}) \Bigg|_{\btheta = \mlepl} \hspace{2em}\text{and} \\
\hat\bJ &= \frac{1}{\sum_h w_h} \sum_h \nabla\log \pL(\btheta \mid \by^{(h)}) \nabla\log \pL(\btheta \mid \by^{(h)})^\top \Bigg|_{\btheta = \mlepl}.
\end{aligned}
$${#eq-hessian}
:::
::: 

# Limited information GOF tests

## Goodness-of-fit

::: {.r-stack}

::: {.absolute left=0 top=70}
```{r}
#| echo: false
knitr::include_graphics("figures/mult_bern2.png")
```
:::

:::: {.absolute left=580 top=70}

- GOF tests are usually constructed by inspecting the fit of the joint probabilities $\hat\pi_r := \pi_r(\hat\btheta)$.

::: {.fragment}
- Most common tests are

   - LR: $X^2 = 2n\sum_r  p_r\log( p_r/\hat\pi_r)$;
   - Pearson: $X^2 = n\sum_r ( p_r - \hat\pi_r)^2 / \hat\pi_r$.
   
  These tests are asymptotically distributed as chi square.
:::

::: {.fragment}
- Likely to face sparsity issues (small or zero cell counts) which distort the approximation to the chi square.
:::

::::

:::

## Limited information goodness-of-fit (LIGOF)

::: {.r-stack}

::: {.absolute left=0 top=70}
```{r}
#| echo: false
knitr::include_graphics("figures/mult_bern2.png")
```
:::

:::: {.absolute left=585 top=60}

::: {.fragment fragment-index=1 .fade-out}
Consider instead the fit of the lower order moments.

- **Univariate**:  $\ \dot\pi_i := \Pr(y_i = 1)$

- **Bivariate**:  $\ \dot\pi_{ij} := \Pr(y_i = 1, y_j=1)$

- Collectively
$$
\bpi_2 = \begin{pmatrix}
\dot\bpi_1 \\
\dot\bpi_2 \\
\end{pmatrix}
=
\begin{pmatrix}
(\dot\pi_1, \dots, \dot\pi_p)^\top \\
\big(\dot\pi_{ij}\big)_{i<j} \\
\end{pmatrix}
$$
This is of dimensions 
$$
S=p + p(p-1)/2 \ll R.
$$

:::

::::


::: {.absolute .fragment  fragment-index=1 .fade-in left=0 top=70}
```{r}
#| echo: false
knitr::include_graphics("figures/mult_bern3.png")
```
:::

::: {.absolute .fragment .fade-in left=0 top=70}
```{r}
#| echo: false
knitr::include_graphics("figures/mult_bern4.png")
```
:::

:::

## Transformation matrix

Consider $\bT_2: \mathbb{R}^R \to \mathbb{R}^S$ defined by $\bpi \mapsto \bpi_2$.
To illustrate, consider $p=3$ so that $R=2^3=8$ and $S=3+3=6$.

$$
\myoverbrace{
\left(
\begin{array}{c}
\dot\pi_1 \\
\dot\pi_2 \\
\dot\pi_3 \\
\hdashline
\dot\pi_{12} \\
\dot\pi_{13} \\
\dot\pi_{23} \\
\end{array} \right)
\vphantom{
\begin{array}{c}
\pi_{000} \\
\pi_{100} \\
\pi_{010} \\
\pi_{001} \\
\pi_{110} \\
\pi_{101} \\
\pi_{011} \\
\pi_{111} \\
\end{array}
}
}{\bpi_2}
=
\myoverbrace{
\left(
\begin{array}{cccccccc}
0 & 1 & 0 & 0 & 1 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 \\
0 & 0 & 0 & 1 & 0 & 1 & 1 & 1 \\
\hdashline
0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
\end{array} \right)
\vphantom{
\begin{array}{c}
\pi_{000} \\
\pi_{100} \\
\pi_{010} \\
\pi_{001} \\
\pi_{110} \\
\pi_{101} \\
\pi_{011} \\
\pi_{111} \\
\end{array}
}
}{\bT_2}
\
\myoverbrace{
\left(
\begin{array}{c}
\pi_{000} \\
\pi_{100} \\
\pi_{010} \\
\pi_{001} \\
\pi_{110} \\
\pi_{101} \\
\pi_{011} \\
\pi_{111} \\
\end{array} \right)
}{\bpi}
$$

::: {.aside}
See @reiser1996analysis and @maydeu2005limited for further details.
:::

## Asymptotic distribution of residuals

::: {#thm-dist-of-residuals .callout-note icon=false title="Theorem"}
Consider the lower order residuals $\hat\be_2 = \bp_2 - \bpi_2(\mlepl)$.
Then as $n\to\infty$,
$$
\sqrt n \, \hat\be_2 \xrightarrow{\text D} {\N}_S\left(\bzero, \bOmega_2\right)
$$ 
where $\bOmega_2 = \left( \bI - \bDelta_2\cH(\btheta)^{-1} \bB(\btheta)  \right) \bSigma_2 \left( \bI - \bDelta_2\cH(\btheta)^{-1} \bB(\btheta)  \right)^\top$, 

::: {.nudge-down}
- $\bSigma_2 = \bT_2\bSigma\bT_2^\top$ (uni \& bivariate multinomial matrix);
- $\bDelta_2 = \bT_2 \big(\partial\pi_r(\btheta) / \partial\theta_k \big)_{r,k}$ (uni \& bivariate derivatives);
- $\cH(\btheta)$ (sensitivity matrix); and
- $\bB(\btheta)$ (some transformation matrix dependent on $\btheta$).
:::
:::

::: {.nudge-up}
To use $\bOmega_2$ in practice, replace with "hat versions" of relevant matrices.
:::

<!-- Using usual Taylor expansion arguments, we can show -->

<!-- $\sqrt n (\bp - \bpi) \xrightarrow{\text D} \N_R(\bzero, \bSigma)$ [CLT].  -->

<!-- - $\sqrt n (\bp_2 - \bpi_2) \xrightarrow{\text D} \N_S(\bzero, \bSigma_2)$ follows from the CLT, where $\bSigma_2 = T_2\bSigma T_2^\top$.  -->

<!-- - Taylor expand MPLE and score function: -->
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \hat\be_2  -->
<!-- = \bp_2 - \bpi_2(\mlepl)  -->
<!-- &\approx \bp_2 - \bpi_2(\btheta) - \bDelta_2(\mlepl - \btheta) \\ -->
<!-- &\approx \bp_2 - \bpi_2(\btheta) - \bDelta_2\cH(\btheta)^{-1}\nabla\pl(\btheta)  \\ -->
<!-- &= \bp_2 - \bpi_2(\btheta) - \bDelta_2 \cH(\btheta)^{-1} \bB(\btheta)  \big(\bp_2 - \bpi_2(\btheta) \big) \\ -->
<!-- &= \left( \bI - \bDelta_2\cH(\btheta)^{-1} \bB(\btheta)  \right) \left( \bp_2 - \bpi_2(\btheta) \right)  -->
<!-- \end{aligned} -->
<!-- $$ -->
<!-- Therefore, $\sqrt n \hat\be_2 \xrightarrow{\text D} {\N}_S\left(\bzero, \bOmega_2\right)$ as $n\to\infty$, where -->
<!-- $$ -->
<!-- \bOmega_2 = \left( \bI - \bDelta_2\cH(\btheta)^{-1} \bB(\btheta)  \right) \bSigma_2 \left( \bI - \bDelta_2\cH(\btheta)^{-1} \bB(\btheta)  \right)^\top -->
<!-- $$ -->

## Distribution of test statistics

LIGOF test statistics generally take the quadratic form 
$$
X^2  = n \hat\be_2^\top \hat\bXi \hat\be_2,
$$
where $\bXi(\hat\btheta) =: \hat\bXi \xrightarrow{\text P} \bXi$ is some $S\times S$ weight matrix. 
Generally, $X^2$ is referred to a chi square distribution under $H_0$, because [@mathai1992quadratic]
$$
X^2 \xrightarrow{\text D} \sum_{s=1}^S \delta_s\chi^2_1 \quad \text{as} \quad n\to\infty,
$$
where the $\delta_s$ are the eigenvalues of $\bM = \bOmega_2\bXi$. Two cases:

1. If $\bM$ is idempotent, then the chi square is exact.
2. Otherwise, it is a sum of scaled chi squares. Can be *approximated* by a chi square with degrees of freedom needing estimation [@cai2006limited].

## Test statistics used

::: columns

::: {.column width=38%}
<br><br><br><br>

::: {.callout-note appearance="simple"}

[$$\begin{gathered}X^2  = n \hat\be_2^\top \hat\bXi \hat\be_2 \\\sqrt n \hat\be_2 \approx {\N}_S (\bzero, \hat\bOmega_2)\end{gathered}$$]{.ubdblue}

:::
:::

::: {.column width=3%}
:::

::: {.column width=59%}
<br>

|   | Name         | $\hat{\bXi}$                              | D.f.  |
|---|--------------|-------------------------------------|-------|
| 1 | Wald         | $\hat{\bOmega}^+_2$                       | $S-m$ |
| 2 | Wald (VCF)   | $\bXi\hat{\bOmega}_2\bXi$                 | $S-m$ |
| 3 | Wald (Diag.) | $\diag(\hat{\bOmega}_2)^{-1}$             | est.  |
| 4 | Pearson      | $\diag(\hat{\bpi}_2)^{-1}$               | est.  |
| [5]{style="color:gray;"} | [RSS]{style="color:gray;"}          | [$\mathbf I$]{style="color:gray;"}                         | [est.]{style="color:gray;"}  |
| [6]{style="color:gray;"} | [Multinomial]{style="color:gray;"}  | [$\hat{\bSigma}_2^{-1}$]{style="color:gray;"} | [est.]{style="color:gray;"}  |
|   |              |                                     |       |

: {.table-ubd}
:::

:::

::: aside
1--@reiser1996analysis; 2--@maydeu2005limited; 4--@bartholomew2002goodness;<br>5,6--@maydeu2008overview.
:::

# Simulation study

## Factor models

::: {.panel-tabset}

### M1: `1F5V`

::::: {.columns}

:::: {.column width=60%}
```{r sem_path_mod1}
#| out-width: 100%
#| fig-height: 8
mod <- "
eta1  =~ y1 + y2 + y3 + y4 + y5
extra =~ y1  + y3 + y5
"
dat <- gen_data_bin(1, seed = 197)
fit <- sem(mod, dat, std.lv = TRUE) %>% suppressWarnings()

semPaths(fit, intercepts = FALSE, #residuals = TRUE, whatLabels = "est",  
         groups = list(c("eta1", paste0("y", c(2, 4))), c("extra")),
         manifests = paste0("y", 5:1),
         latents = c("extra", "eta1"),
         # color = c("#28BBECFF", "#7A0403FF"),
         color = c("#00A5CF", "#DE1A1A"),
         node.width = 0.9, nCharNodes = 0, sizeMan = 12, sizeLat = 12, 
         thresholds = FALSE, rotation = 3, exoCov = FALSE)
```
::::

:::: {.column width=40%}
```{r}
print_theta_vals <- function(model_no) {
  # Lambda
  cat("Loadings\n")
  Lambda <- lavaan.bingof:::get_Lambda(model_no)
  Lambda <- as(Lambda, "sparseMatrix")
  print(Lambda)
  cat("\n")
  # tau
  cat("Thresholds\n")
  print(lavaan.bingof:::get_tau(model_no))
  cat("\n")
  # rho
  cat("Factor correlations\n")
  Psi <- lavaan.bingof:::get_Psi(model_no)
  # Psi <- as(Psi, "sparseMatrix")
  print(Psi)
}
options(width=30)
print_theta_vals(1)
```
::::

:::::

### M2: `1F8V`

::::: {.columns}

:::: {.column width=60%}
```{r sem_path_mod2}
#| out-width: 100%
#| fig-height: 8
mod <- "
eta1  =~ y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8
extra =~ y1 + y3 + y4 + y5 + y7 + y8
# eta1 ~~ 0*extra
# eta1 ~~ 1*eta1
"
dat <- gen_data_bin(2, seed = 197)
fit <- sem(mod, dat, std.lv = TRUE) %>% suppressWarnings()

semPaths(fit, intercepts = FALSE, #residuals = TRUE, whatLabels = "est",  
         groups = list(c("eta1", paste0("y", c(2, 6))), c("extra")),
         manifests = paste0("y", 8:1),
         latents = c("extra", "eta1"),
         # color = c("#28BBECFF", "#7A0403FF"),
         color = c("#00A5CF", "#DE1A1A"),
         node.width = 0.9, nCharNodes = 0, sizeMan = 10, sizeLat = 12, 
         thresholds = FALSE, rotation = 3, exoCov = FALSE)
# "#30123BFF" "#28BBECFF" "#A2FC3CFF" "#FB8022FF" "#7A0403FF"
```
::::

:::: {.column width=40%}
```{r}
options(width=30)
print_theta_vals(2)
```
::::

:::::

### M3: `1F15V`

::::: {.columns}

:::: {.column width=60%}
```{r sem_path_mod3}
#| out-width: 100%
#| fig-height: 8
mod <- "
eta1  =~ y1 +  y2 +  y3 +  y4 +  y5 + 
         y6 +  y7 +  y8 +  y9 + y10 +
        y11 + y12 + y13 + y14 + y15
extra =~ y1 +  y3 +  y4 +  y5 + 
         y6 +  y7 +  y9 + y10 +
        y11 + y12 + y13 + y15
# eta1 ~~ 0*extra
# eta1 ~~ 1*eta1
"
dat <- gen_data_bin(3, seed = 197)
fit <- sem(mod, dat, std.lv = TRUE) %>% suppressWarnings()

semPaths(fit, intercepts = FALSE, #residuals = TRUE, whatLabels = "est",  
         groups = list(c("eta1", paste0("y", c(2, 8, 14))), c("extra")),
         manifests = paste0("y", 15:1),
         latents = c("extra", "eta1"),
         # color = c("#28BBECFF", "#7A0403FF"),
         color = c("#00A5CF", "#DE1A1A"),
         nCharNodes = 0, sizeMan = 4, sizeLat = 12, 
         thresholds = FALSE, rotation = 3, exoCov = FALSE)
# "#30123BFF" "#28BBECFF" "#A2FC3CFF" "#FB8022FF" "#7A0403FF"
```
::::

:::: {.column width=40%}
```{r}
print_theta_vals(3)
```
::::

:::::

### M4: `2F10V`

::::: {.columns}

:::: {.column width=60%}

```{r sem_path_mod4}
#| out-width: 100%
#| fig-height: 8
mod <- "
eta1  =~ y1 +  y2 +  y3 +  y4 +  y5 
eta2  =~ y6 +  y7 +  y8 +  y9 + y10 
extra  =~ y1 +  y2 +  y3 +  y4 +  y5 + 
         y6 +  y7 +  y8 +  y9 + y10
eta1 ~~ 0*extra
eta2 ~~ 0*extra
# eta1 ~~ 0.3*eta2
"
dat <- gen_data_bin(4, seed = 197)
fit <- sem(mod, dat, std.lv = TRUE) %>% suppressWarnings()

semPaths(fit, intercepts = FALSE, what = "par", 
         whatLabels = "hide", #residuals = TRUE, whatLabels = "est",  
         groups = list(c("eta1", paste0("y", 1:5)), 
                       c("eta2", paste0("y", 6:10)), 
                       c("extra")),
         manifests = paste0("y", 10:1),
         latents = c("extra", "eta2", "eta1"),
         # color = c("#30123BFF", "#A2FC3CFF", "#7A0403FF"),
         color = c("#00A5CF", "#29BF12", "#DE1A1A"),
         node.width = 0.9, nCharNodes = 0, sizeMan = 6, sizeLat = 9, 
         thresholds = FALSE, rotation = 3, weighted = FALSE)
```
::::

:::: {.column width=40%}
```{r}
print_theta_vals(4)
```
::::

:::::

### M5: `3F15V`

::::: {.columns}

:::: {.column width=60%}
```{r sem_path_mod5}
#| out-width: 100%
#| fig-height: 8
mod <- "
eta1  =~  y1 +  y2 +  y3 +  y4 +  y5 
eta2  =~  y6 +  y7 +  y8 +  y9 + y10 
eta3  =~ y11 + y12 + y13 + y14 + y15
extra  =~ y1 +  y2 +  y3 +  y4 +  y5 + 
          y6 +  y7 +  y8 +  y9 + y10 +
         y11 + y12 + y13 + y14 + y15
eta1 ~~ 0*extra
eta2 ~~ 0*extra
eta3 ~~ 0*extra
"
dat <- gen_data_bin(5, seed = 197)
fit <- sem(mod, dat, std.lv = TRUE) %>% suppressWarnings()

semPaths(fit, intercepts = FALSE, what = "par", 
         whatLabels = "hide", #residuals = TRUE, whatLabels = "est",  
         groups = list(c("eta1", paste0("y", 1:5)), 
                       c("eta2", paste0("y", 6:10)), 
                       c("eta3", paste0("y", 11:15)), 
                       c("extra")),
         manifests = paste0("y", 15:1),
         latents = c("extra", "eta3", "eta2", "eta1"),
         # color = c("#30123BFF", "#1AE4B6FF", "#FABA39FF", "#7A0403FF"),
         color = c("#00A5CF", "#29BF12", "#FFBF00", "#DE1A1A"),
         node.width = 0.9, nCharNodes = 0, sizeMan = 5, sizeLat = 9, 
         thresholds = FALSE, rotation = 3, weighted = FALSE)
```
::::

:::: {.column width=40%}
```{r}
print_theta_vals(5)
```
::::

:::::

:::

## Experiment 1: Informative sampling

- Using M1: `1F5V`, generate a fixed population size of $N$. Then, assign each unit $h$ a probability of selection as follows:
$$
w_h^{-1} = \frac{1}{1 + \exp(y_1^*)}.
$$
Larger values of $y_1^*$ result in smaller probabilities of selection.

- Sample $n\in\{500, 1000, 5000\}$ units from a population of size $N=n/0.01$ which ensures no need for FPC factor [@lumley2004analysis].

- In repeated sampling ($B=1000$), interested in performance of PMLE vis-Ã -vis
  - Bias
  - Coverage for 95% CI
  - SD/SE ratio

## Informative sampling (results) {.scrollable}

### Bias

```{r}
load("R/sims_uneq_samp_mod5.Rdata")
tab_bias_gt$`_data`$PML_500a <- abs(tab_bias_gt$`_data`$PML_500)
tab_bias_gt$`_data`$PMLW_500a <- abs(tab_bias_gt$`_data`$PMLW_500)
tab_bias_gt$`_data`$PML_1000a <- abs(tab_bias_gt$`_data`$PML_1000)
tab_bias_gt$`_data`$PMLW_1000a <- abs(tab_bias_gt$`_data`$PMLW_1000)
tab_bias_gt$`_data`$PML_5000a <- abs(tab_bias_gt$`_data`$PML_5000)
tab_bias_gt$`_data`$PMLW_5000a <- abs(tab_bias_gt$`_data`$PMLW_5000)

tab_bias_gt |>
  fmt_number(columns = -"truth", decimals = 2) |>
  tab_options(
    table.font.size = "22px",
    # table.layout = "auto",
    quarto.disable_processing = TRUE
  ) |>
  data_color(
    columns = c("PML_500a", "PMLW_500a", "PML_1000a", "PMLW_1000a", "PML_5000a", "PMLW_5000a"),
    target_columns = c("PML_500", "PMLW_500", "PML_1000", "PMLW_1000", "PML_5000", "PMLW_5000"),
    method = "numeric",
    palette = "plasma",
    domain = c(0, 0.32),
    alpha = 0.9
  ) |>
  cols_width(
    3:8 ~ px(120)
  ) 
```

## Informative sampling (results)  {.scrollable}

### Coverage and SD/SE ratio 

```{r}
tab_se_gt$`_data`$cov_PML_500a <- abs(tab_se_gt$`_data`$cov_PML_500 - 0.95)
tab_se_gt$`_data`$cov_PMLW_500a <- abs(tab_se_gt$`_data`$cov_PMLW_500 - 0.95)
tab_se_gt$`_data`$cov_PML_1000a <- abs(tab_se_gt$`_data`$cov_PML_1000 - 0.95)
tab_se_gt$`_data`$cov_PMLW_1000a <- abs(tab_se_gt$`_data`$cov_PMLW_1000 - 0.95)
tab_se_gt$`_data`$cov_PML_5000a <- abs(tab_se_gt$`_data`$cov_PML_5000 - 0.95)
tab_se_gt$`_data`$cov_PMLW_5000a <- abs(tab_se_gt$`_data`$cov_PMLW_5000 - 0.95)

tab_se_gt$`_data`$ratio_PML_500a <- log(tab_se_gt$`_data`$ratio_PML_500) |> abs()
tab_se_gt$`_data`$ratio_PMLW_500a <- log(tab_se_gt$`_data`$ratio_PMLW_500) |> abs()
tab_se_gt$`_data`$ratio_PML_1000a <- log(tab_se_gt$`_data`$ratio_PML_1000) |> abs()
tab_se_gt$`_data`$ratio_PMLW_1000a <- log(tab_se_gt$`_data`$ratio_PMLW_1000) |> abs()
tab_se_gt$`_data`$ratio_PML_5000a <- log(tab_se_gt$`_data`$ratio_PML_5000) |> abs()
tab_se_gt$`_data`$ratio_PML_5000a[1:5] <- tab_se_gt$`_data`$ratio_PML_5000a[1:5] + 0.8
tab_se_gt$`_data`$ratio_PMLW_5000a <- log(tab_se_gt$`_data`$ratio_PMLW_5000) |> abs()

tab_se_gt |>
  tab_options(
    table.font.size = "22px"
  ) |>
  data_color(
    columns = c("cov_PML_500a", "cov_PMLW_500a", "cov_PML_1000a", "cov_PMLW_1000a", "cov_PML_5000a", "cov_PMLW_5000a"),
    target_columns = c("cov_PML_500", "cov_PMLW_500", "cov_PML_1000", "cov_PMLW_1000", "cov_PML_5000", "cov_PMLW_5000"),
    method = "numeric",
    palette = "plasma",
    domain = c(0, 0.95),
    alpha = 0.9
  ) |>
  data_color(
    columns = c("ratio_PML_500a", "ratio_PMLW_500a", "ratio_PML_1000a", "ratio_PMLW_1000a", "ratio_PML_5000a", "ratio_PMLW_5000a"),
    target_columns = c("ratio_PML_500", "ratio_PMLW_500", "ratio_PML_1000", "ratio_PMLW_1000", "ratio_PML_5000", "ratio_PMLW_5000"),
    method = "numeric",
    palette = "plasma",
    domain = c(0, 2.65),
    alpha = 0.9
  )
```

## Experiment 2: Educational survey

Simulate a population of 10^6^ students clustered within classrooms and stratified by school type (correlating with abilities). 

::: {.columns}

::: {.column width=60%}
```{r}
#| label: pop_byregion_vars
#| out-width: 100%
#| fig-height: 2.3
#| fig-width: 5
options(width = 80)
pop <- make_population(model_no = 1, seed = 123)
prop1 <- summarise(pop, across(starts_with("y"), mean)) %>% unlist()

pop %>%
  select(type, starts_with("y")) %>%
  pivot_longer(-type) %>%
  mutate(value = factor(value),
         type = factor(type, levels = c("C", "B", "A"))) %>%
  ggplot(aes(type, fill = value)) +
  geom_bar(position = "fill", alpha = 0.8) +
  geom_hline(data = tibble(y = prop1, name = names(prop1)),
             aes(yintercept = prop1, linetype = "Pop. Avg.")) +
  scale_linetype_manual(values = "dashed", name = NULL) +
  facet_grid(~ name, scales = "free_y") +
  coord_flip() +
  labs(x = "School type", y = "Population proportion", fill = "Response") +
  theme_bw() +
  theme(legend.position = "top", 
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.title = element_text(size = 9)) +
  scale_fill_manual(values = c("#00A5CF", "#DE1A1A"))
  # scale_fill_manual(values = c("#b10f2e", "#325494"))
```
:::

::: {.column width=40%}

```{r}
n_schools <-
  pop |>
  summarise(
    N = n_distinct(school),
    .by = c(type)
  )

n_classes <-
  pop |>
  summarise(
    Classes = n_distinct(class),
    .by = c(type, school)
  ) |>
  summarise(Classes = mean(Classes), .by = type)

class_size <-
  pop |>
  summarise(
    class_size = n(),
    .by = c(type, school, class)
  ) |>
  summarise(`Avg. class size` = mean(class_size), .by = type)

n_schools |>
  left_join(n_classes) |>
  left_join(class_size) |>
  gt() |>
  fmt_number(columns = -"N", decimals = 1) |>
  cols_label(N = md("$N$"), type = "Type") |>
  tab_options(
    table.font.size = "30px",
    quarto.disable_processing = TRUE
  )
```

$\text{ICC} \in (0.05, 0.60)$
<!-- ($\frac{\sigma_u^2}{\sigma_u^2 + \sigma_\epsilon^2}$)  -->
:::

:::

::: {.nudge-up}

- **Cluster sample:** Sample $n_C$ schools using PPS, then sample 1 classroom via SRS, then select all students in classroom.
- **Stratified cluster sample:** For each stratum, sample $n_S$ schools using SRS, then sample 1 classroom via SRS, then select all students in classroom.

:::

## Educational survey (results) 

### Type I error rates ($\alpha = 5\%$, $n=5000$)

```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 4.4
load("R/p_type1.Rdata")
p_type1
```

## Educational survey (results) 

### Power analysis ($\alpha = 5\%$)

```{r}
#| include: false
load("R/p_power.RData")
```

::: {.absolute .fragment .fade-out fragment-index=1 left=0 top=95}
```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 4.4
p_power1 #
```
:::

::: {.absolute .fragment .fade-in fragment-index=1 left=0 top=95}
```{r}
#| out-width: 100%
#| fig-width: 8
#| fig-height: 4.4
p_power2
```
:::

# Summary

## Conclusions

â¡ **PML estimation** offers a *computationally efficient* alternative that mitigates some challenges inherent in the **UV approach** for binary factor models.

ð§© For samples with **unequal probability of selection**, sampling weights are **easily incorporated** in the PML estimation routine.

ð **Sparsity impairs the *dependability* ** of GOF tests, but are circumvented by considering lower order statistics. 

- â Generally all tests have acceptable Type I errors, except WaldDiag test.
- ð ï¸ Wald and Pearson tests need $\hat{\bOmega}_2^{-1}$, but WaldVCF and WaldDiag do not.
- â ï¸ Ignoring sampling weights may lead to inflated Type I errors and lower power.
- ð **Pearson test** seems the most robust. 

## Software

::: {.nudge-up-small}
R software implementation in `{lavaan} (>= 0.6-17)` to carry out weighted PML, and separately in `{lavaan.bingof}` for the LIGOF tests.

```{r}
#| include: false
options(width = 80)
```

```{r}
#| echo: true
#| code-line-numbers: "|5,6|8"
fit <- lavaan::sem(
  model = "eta1 =~ y1 + y2 + y3 + y4 + y5",
  data = lavaan.bingof::gen_data_bin_wt(n = 1000),
  std.lv = TRUE,
  estimator = "PML",
  sampling.weights = "wt"
)
lavaan.bingof::all_tests(fit)
```
:::

::: aside
{{< fa brands github  >}} https://github.com/haziqj/lavaan.bingof
:::

# Thanks! {.transition-slide-ubdblue}

[`https://haziqj.ml/plgof-nus`](https://haziqj.ml/plgof-nus)

## References {.appendix}